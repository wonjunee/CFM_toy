# -*- coding: utf-8 -*-
"""CFM example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nsdvH9WmgxJHB8agftXLQI4fSHIHjglT
"""

import tqdm
import random
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import os
import torch
import torch.nn as nn
import torch.nn.functional as F

torch.set_default_dtype(torch.float)
seed = 0
np.random.seed(seed)
torch.manual_seed(seed)

save_fig_path = f'out'
if not os.path.exists(save_fig_path):
    os.makedirs(save_fig_path)
print(f"saving images in {save_fig_path}")

save_data_path = f'data'
os.makedirs(save_data_path, exist_ok=True)
print(f"saving data in {save_data_path}")


cuda = True if torch.cuda.is_available() else False
device = torch.device("cuda" if cuda else "cpu")

num_epochs = 1_000_000
lr = 1e-5
Nt = 200
zdim = 2

def create_data_mu(sample_size, input_dim=2):
    # a = torch.randn(sample_size//2, input_dim); a[:,0] *= 0.01; a[:,0] += 2
    # b = torch.randn(sample_size//2, input_dim); b[:,0] *= 0.01; b[:,0] -= 2
    # shuffle_indices = torch.randperm(sample_size)
    # return torch.cat((a,b),0)[shuffle_indices]

    a = torch.randn(sample_size, input_dim); a[:,1] *= 0.01;
    a[:,1] += a[:,0]
    return a

def get_latent(N, zdim, device):
    """
    Generates a tensor of random latent variables.

    Args:
        N (int): The number of latent vectors to generate.
        zdim (int): The dimensionality of each latent vector.
        device (torch.device): The device on which to create the tensor (e.g., 'cpu' or 'cuda').

    Returns:
        torch.Tensor: A tensor of shape (N, zdim) containing random values sampled 
                        from a standard normal distribution, created on the specified device.
    """
    z = torch.randn(N,zdim,device=device)
    z[:,0] *= 0.01
    return z


class Net(torch.nn.Module):
    def __init__(self, zdim = 100):
        super(Net, self).__init__()
        self.zdim = zdim
        dd = 500

        self.model = nn.Sequential(
            nn.Linear(zdim+1, dd),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(dd, dd),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(dd, dd),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(dd, zdim),
        )

    def forward(self, x, t):
        if isinstance(t, float):
            t = torch.ones(x.shape[0],1,device=x.device) * t

        xt = torch.concat((x,t),1)
        return self.model(xt)

# for interpolating the trajectories from ODE
def interpolate(net, x, Nt=5, Nt_list=None, inv=False):
    if Nt_list is None:
        Nt_list = np.ones(Nt)/Nt

    xx = x * 1.0
    tau = 1.0/Nt
    arr = torch.zeros((Nt+1,x.shape[0],x.shape[1]))
    x1 = xx
    arr = [x1.detach()]
    for i in range(Nt):
        v1 = net(x1,i/Nt)
        x1 = x1 + v1 * 1.0/Nt + torch.randn(x1.shape, device=x1.device) * 0.1/(Nt * (i+1))
        arr.append(x1.detach())
    return arr

batch_size = 500
sample_size = 30_000
xdata = create_data_mu(sample_size=sample_size, input_dim=zdim)
dataloader = torch.utils.data.DataLoader(xdata, batch_size=batch_size, shuffle=True,)

net       = Net(zdim=zdim).to(device)
optR      = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.5, 0.999))

pbar = tqdm.tqdm(range(num_epochs))
fig_count = 0
for total_count in pbar:
    x = next(iter(dataloader))
    optR.zero_grad()
    z = get_latent(batch_size,zdim,device)
    t = torch.rand(z.shape[0], 1, device=z.device)
    xt = (1-t) * z + t * x
    ut = x - z
    vt = net(xt, t.reshape((-1,1)))
    loss = (vt - ut).pow(2).mean()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)
    optR.step()
    pbar.set_description(f'loss: {loss:9.2e}')

    # for plotting
    if (total_count+1) % 1000 ==0:
        with torch.no_grad():
            # Create a figure and a GridSpec object for layout
            n_inter = 5
            fig = plt.figure(figsize=(int(3*(3+n_inter)), 3))
            gs = GridSpec(1, 3+n_inter, figure=fig)

            N = 2_000
            x = xdata[:N].detach()
            z = get_latent(N,zdim,device).detach()
            Nt = 20

            ax_mu      = fig.add_subplot(gs[0, 0])
            ax_traj    = fig.add_subplot(gs[0, 1])
            ax_target  = fig.add_subplot(gs[0, 2])

            ax_inter = []
            for i in range(n_inter):
                ax_inter.append(fig.add_subplot(gs[0,i+3]))

            Nt_list = np.ones(Nt)/Nt
            skip = Nt//n_inter
            ax_mu.scatter(z.cpu()[:,0],z.cpu()[:,1],marker='o',color='tab:green', alpha=0.05)
            ax_mu.scatter(x.cpu()[:,0],x.cpu()[:,1],marker='o',color='tab:blue', alpha=0.1)

            Rz_arr = interpolate(net, z, Nt, Nt_list=Nt_list)

            for i in range(0, Nt+1, skip):
                XX = Rz_arr[i].cpu()
                t = i / Nt
                ind = i//skip
                color = (t, 0, 1 - t)  # blue at t=0, red at t=1
                # If not the last iteration, connect points to the next iteration
                if i < Nt+1 - skip:
                    ind_next = i+skip
                    XX_next = Rz_arr[ind_next].cpu()
                    for j in range(XX.shape[0]):
                        ax_traj.plot([XX[j, 0], XX_next[j, 0]], [XX[j, 1], XX_next[j, 1]], color=color, alpha=0.2)
                    ax_inter[ind].scatter(z.cpu()[:,0],z.cpu()[:,1],marker='o',color='tab:green', alpha=0.01)
                    ax_inter[ind].scatter(x.cpu()[:,0],x.cpu()[:,1],marker='o',color='tab:red', alpha=0.01)
                    ax_inter[ind].scatter(XX_next[:, 0], XX_next[:, 1], alpha=0.2)
                    ax_inter[ind].set_xticks([]);ax_inter[ind].set_yticks([]);ax_inter[ind].set_aspect('equal')

            ax_traj.set_xticks([])
            ax_traj.set_yticks([])
            ax_traj.set_title('fm')
            ax_target.scatter( x.cpu()[:,0],  x.cpu()[:,1], marker='o', color='tab:blue', alpha=0.1)
            ax_target.scatter(XX.cpu()[:,0], XX.cpu()[:,1], marker='x', color='tab:orange', alpha=0.1)

            ax_mu.set_aspect('equal')
            ax_traj.set_aspect('equal')
            ax_target.set_aspect('equal')

            # Adjust layout
            plt.subplots_adjust(left=0.03, right=0.98, bottom=0.05, top=0.95)

            # Save the figure
            plt.savefig(f'{save_fig_path}/G-{fig_count}.png')
            plt.savefig(f'{save_fig_path}/0000interpolate-status.png')
            plt.close('all')

            fig_count += 1

